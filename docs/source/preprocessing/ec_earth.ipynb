{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC-Earth data (challenging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import yaml\n",
    "\n",
    "from preprocessing import (\n",
    "    resample,\n",
    "    get_stable_fluxes,\n",
    "    get_vertical_transport,\n",
    "    get_grid_info,\n",
    "    join_levels,\n",
    "    repeat_upper_level,\n",
    "    get_new_target_levels,\n",
    "    interpolate,\n",
    ")\n",
    "\n",
    "\n",
    "# Set constants\n",
    "g = 9.80665  # [m/s2]\n",
    "density_water = 1000  # [kg/m3]\n",
    "\n",
    "# Read case configuration\n",
    "with open(\"cases/ec-earth.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Parse input from config file\n",
    "# Reassignment not strictly needed but improves readability for often used vars\n",
    "input_folder = config[\"input_folder\"]\n",
    "name_of_run = config[\"name_of_run\"]\n",
    "divt = config[\"divt\"]\n",
    "count_time = config[\"count_time\"]\n",
    "latnrs = np.arange(config[\"latnrs\"])\n",
    "lonnrs = np.arange(config[\"lonnrs\"])\n",
    "\n",
    "\n",
    "def _get_input_data(variable, date, latnrs, lonnrs):\n",
    "    \"\"\"Get input data for variable.\"\"\"\n",
    "    filename = f\"{name_of_run}{variable}_{date.year}{date.month:02d}_NH.nc\"\n",
    "    filepath = os.path.join(config[\"input_folder\"], filename)\n",
    "    return (\n",
    "        xr.open_dataset(filepath)\n",
    "        .sel(time=date.strftime(\"%Y%m%d\"))\n",
    "        .isel(lat=latnrs, lon=lonnrs)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_input_data(variable, date, latnrs, lonnrs):\n",
    "    \"\"\"Workaround for keeping original timestamps crossing midnight.\"\"\"\n",
    "    day1 = _get_input_data(variable, date, latnrs, lonnrs)\n",
    "    day2 = _get_input_data(variable, date + dt.timedelta(days=1), latnrs, lonnrs)\n",
    "    days = xr.concat([day1, day2], dim=\"time\")\n",
    "    if len(days.time) < 9:\n",
    "        return days.isel(time=slice(0, 5)).isel(lat=latnrs, lon=lonnrs)\n",
    "    else:\n",
    "        return days.isel(time=slice(0, 10)).isel(lat=latnrs, lon=lonnrs)\n",
    "\n",
    "\n",
    "datelist = pd.date_range(\n",
    "    start=config[\"start_date\"], end=config[\"end_date\"], freq=\"d\", inclusive=\"left\"\n",
    ")\n",
    "\n",
    "for date in datelist:\n",
    "    print(date)\n",
    "\n",
    "    # Load data\n",
    "    time_3h = slice(1, None, 2)\n",
    "    times_lnsp = slice(None, None, 2)  # TODO should be slice(1, None, 2) ??\n",
    "\n",
    "    q = get_input_data(\"Q\", date, latnrs, lonnrs).Q\n",
    "    u = get_input_data(\"U\", date, latnrs, lonnrs).U\n",
    "    v = get_input_data(\"V\", date, latnrs, lonnrs).V\n",
    "    q2m = get_input_data(\"Q2M\", date, latnrs, lonnrs).Q2M.isel(time=time_3h)\n",
    "    lnsp = get_input_data(\"LNSP\", date, latnrs, lonnrs).LNSP.isel(time=times_lnsp)\n",
    "    u10 = get_input_data(\"U10\", date, latnrs, lonnrs).U10M.isel(time=time_3h)\n",
    "    v10 = get_input_data(\"V10\", date, latnrs, lonnrs).V10M.isel(time=time_3h)\n",
    "\n",
    "    sp = np.exp(lnsp)  # log(sp) --> sp (Pa)\n",
    "\n",
    "    # Create pressure array with the same dimensions as u, q, and v\n",
    "    p = u.lev.broadcast_like(u)\n",
    "\n",
    "    # Get the most out of the data we have\n",
    "    u = join_levels(u, u10)  # bottom level will be set to 1000 00 Pa\n",
    "    v = join_levels(v, v10)\n",
    "    q = join_levels(q, q2m)\n",
    "    p = join_levels(p, sp.squeeze())\n",
    "\n",
    "    u = repeat_upper_level(u)  # top level will be set to 100 00 Pa\n",
    "    v = repeat_upper_level(v)\n",
    "    q = repeat_upper_level(q, fill_value=0)\n",
    "    p = repeat_upper_level(p, fill_value=0)\n",
    "\n",
    "    # Mask data where the pressure level is higher than sp - 1000\n",
    "    # as there is no valid data at those points\n",
    "    # and we don't need a second layer that is very close to the surface\n",
    "    mask = p > sp.values - 1000\n",
    "    mask[:, 0, :, :] = False  # don't mask bottom (surface pressure values)\n",
    "    mask[:, -1, :, :] = False  # don't mask top (\"ghost cells\"?)\n",
    "\n",
    "    # TODO convert to nan instead of numpy masked array?\n",
    "    # u_masked = u.where(mask)\n",
    "    u = np.ma.masked_array(u, mask=mask)\n",
    "    v = np.ma.masked_array(v, mask=mask)\n",
    "    q = np.ma.masked_array(q, mask=mask)\n",
    "    p = np.ma.masked_array(p, mask=mask)\n",
    "\n",
    "    ####\n",
    "    # Get grid info\n",
    "    dummy = xr.open_dataset(config[\"land_sea_mask\"])\n",
    "    lat = dummy.LAT.values[444:711][::-1]  # [degrees north]\n",
    "    lon = dummy.XAS.values[934:1378][::-1]  # [degrees east]\n",
    "    a_gridcell, l_ew_gridcell, l_mid_gridcell = get_grid_info(lat, lon)\n",
    "\n",
    "    # Reverse engineered ~ model level 47 which corresponds with about ~800 hPa\n",
    "    p_boundary = 0.72878581 * sp.values + 7438.803223\n",
    "    new_pressure_levels = get_new_target_levels(sp, p_boundary, n_levels=40)\n",
    "\n",
    "    print(\"before interpolation loop\", dt.datetime.now().time())\n",
    "    uq_boundaries = interpolate(u * q, p, new_pressure_levels, type=\"cubic\")\n",
    "    vq_boundaries = interpolate(v * q, p, new_pressure_levels, type=\"cubic\")\n",
    "    q_boundaries = interpolate(q, p, new_pressure_levels, type=\"linear\")\n",
    "\n",
    "    # Integrate specific humidity to get the (total) column water (vapor) and calculate horizontal moisture fluxes\n",
    "    q_midpoints = 0.5 * (q_boundaries[:, 1:, :, :] + q_boundaries[:, :-1, :, :])\n",
    "    uq_midpoints = 0.5 * (uq_boundaries[:, 1:, :, :] + uq_boundaries[:, :-1, :, :])\n",
    "    vq_midpoints = 0.5 * (vq_boundaries[:, 1:, :, :] + vq_boundaries[:, :-1, :, :])\n",
    "    # for p we do not calculate the mean pressure but the pressure difference between two levels!\n",
    "    p_diff = np.maximum(\n",
    "        0, new_pressure_levels[:, :-1, :, :] - new_pressure_levels[:, 1:, :, :]\n",
    "    )  # the maximum statement is necessary to avoid negative humidity values\n",
    "\n",
    "    # eastward and northward fluxes\n",
    "    fa_e_p = uq_midpoints * p_diff / g\n",
    "    fa_n_p = vq_midpoints * p_diff / g\n",
    "\n",
    "    column_water_vapor = q_midpoints * p_diff / g  # [kg/m2]\n",
    "\n",
    "    # summed over the vertical\n",
    "    total_column_water_vapor = np.squeeze(np.sum(column_water_vapor, 1))\n",
    "\n",
    "    mask = np.where(new_pressure_levels > p_boundary, 1.0, 0.0)\n",
    "\n",
    "    vapor_lower = np.sum(mask[:, :-1, :, :] * q_midpoints * p_diff / g, axis=1)\n",
    "    vapor_upper = np.sum((1 - mask[:, :-1, :, :]) * q_midpoints * p_diff / g, axis=1)\n",
    "\n",
    "    fa_e_lower = np.sum(mask[:, :-1, :, :] * fa_e_p, axis=1)  # kg*m-1*s-1\n",
    "    fa_n_lower = np.sum(mask[:, :-1, :, :] * fa_n_p, axis=1)  # kg*m-1*s-1\n",
    "    fa_e_upper = np.sum((1 - mask[:, :-1, :, :]) * fa_e_p, axis=1)  # kg*m-1*s-1\n",
    "    fa_n_upper = np.sum((1 - mask[:, :-1, :, :]) * fa_n_p, axis=1)  # kg*m-1*s-1\n",
    "\n",
    "    vapor_total = vapor_upper + vapor_lower\n",
    "\n",
    "    check = np.sum(total_column_water_vapor - vapor_total)\n",
    "    print(f\"Check calculation water vapor, this value should be zero: {check}\")\n",
    "\n",
    "    # water volumes\n",
    "    w_upper = vapor_upper * a_gridcell[None, ...] / density_water  # m3\n",
    "    w_lower = vapor_lower * a_gridcell[None, ...] / density_water  # m3\n",
    "\n",
    "    ## Get E and P\n",
    "    # (accumulated after the forecast at 00.00 and 12.00 by steps of 3 hours in time\n",
    "    evaporation = get_input_data(\"EVAP\", date, latnrs, lonnrs).E  # m\n",
    "    precipitation = get_input_data(\"TP\", date, latnrs, lonnrs).TP  # m\n",
    "\n",
    "    # change sign convention to all positive, transfer negative (originally positive) values of evap to precip\n",
    "    precipitation = np.maximum(precipitation + np.maximum(evaporation, 0), 0)\n",
    "    evaporation = np.abs(np.minimum(evaporation, 0))\n",
    "\n",
    "    # calculate volumes\n",
    "    evap = (evaporation * a_gridcell).values\n",
    "    precip = (precipitation * a_gridcell).values\n",
    "\n",
    "    # put data on a smaller time step\n",
    "    evap = resample(evap, divt / 2, count_time, method=\"bfill\")[:-1]\n",
    "    precip = resample(precip, divt / 2, count_time, method=\"bfill\")[:-1]\n",
    "    w_upper = resample(w_upper, divt, count_time, method=\"interp\")\n",
    "    w_lower = resample(w_lower, divt, count_time, method=\"interp\")\n",
    "    fa_e_upper = resample(fa_e_upper, divt, count_time, method=\"interp\")[:-1]\n",
    "    fa_e_lower = resample(fa_e_lower, divt, count_time, method=\"interp\")[:-1]\n",
    "    fa_n_upper = resample(fa_n_upper, divt, count_time, method=\"interp\")[:-1]\n",
    "    fa_n_lower = resample(fa_n_lower, divt, count_time, method=\"interp\")[:-1]\n",
    "\n",
    "    # convert to m3   [kg*m^-1 * s^-1 * s * m * kg^-1 * m^3] = [m3]\n",
    "    total_seconds = config[\"timestep\"] / config[\"divt\"]\n",
    "    fa_e_upper *= total_seconds * (l_ew_gridcell / density_water)\n",
    "    fa_e_lower *= total_seconds * (l_ew_gridcell / density_water)\n",
    "    fa_n_upper *= total_seconds * (l_mid_gridcell[None, :, None] / density_water)\n",
    "    fa_n_lower *= total_seconds * (l_mid_gridcell[None, :, None] / density_water)\n",
    "\n",
    "    # stabilize horizontal fluxes\n",
    "    fa_e_upper, fa_n_upper = get_stable_fluxes(fa_e_upper, fa_n_upper, w_upper)\n",
    "    fa_e_lower, fa_n_lower = get_stable_fluxes(fa_e_lower, fa_n_lower, w_lower)\n",
    "\n",
    "    # determine the vertical moisture flux\n",
    "    fa_vert = get_vertical_transport(\n",
    "        fa_e_upper, fa_e_lower, fa_n_upper, fa_n_lower, evap, precip, w_upper, w_lower\n",
    "    )\n",
    "\n",
    "    # Save preprocessed data\n",
    "    filename = f\"{date.strftime('%Y-%m-%d')}_fluxes_storages.nc\"\n",
    "    output_path = os.path.join(config[\"interdata_folder\"], filename)\n",
    "    xr.Dataset(\n",
    "        {  # TODO: would be nice to add coordinates and units as well\n",
    "            \"fa_e_upper\": ([\"time\", \"lat\", \"lon\"], fa_e_upper),\n",
    "            \"fa_n_upper\": ([\"time\", \"lat\", \"lon\"], fa_n_upper),\n",
    "            \"fa_e_lower\": ([\"time\", \"lat\", \"lon\"], fa_e_lower),\n",
    "            \"fa_n_lower\": ([\"time\", \"lat\", \"lon\"], fa_n_lower),\n",
    "            \"evap\": ([\"time\", \"lat\", \"lon\"], evap),\n",
    "            \"precip\": ([\"time\", \"lat\", \"lon\"], precip),\n",
    "            \"w_upper\": ([\"time2\", \"lat\", \"lon\"], w_upper),  # note different time\n",
    "            \"w_lower\": ([\"time2\", \"lat\", \"lon\"], w_lower),  # note different time\n",
    "            \"fa_vert\": ([\"time\", \"lat\", \"lon\"], fa_vert),\n",
    "        }\n",
    "    ).to_netcdf(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('wam2layers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a20d0a8427cf157660a5f7831b5ea181b2261d0622e99d9dfb33ceae678266e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
